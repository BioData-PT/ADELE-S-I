#!/bin/sh

set -u -e

myself=$0

# Default values for global options.  Values in the environment override
# these hard-coded defaults, and values set via command line options
# override these later.
MQ_CREDENTIALS=${MQ_CREDENTIALS-test:test}	# --mq-credentials user:pass
MQ_URL=${MQ_URL-http://localhost:15672}		# --mq-url URL
SDA_CONFIG=${SDA_CONFIG-s3cmd.conf}		# --sda-config pathname
SDA_KEY=${SDA_KEY-crypt4gh_key.pub}		# --sda-key pathname

# Allow a user to use the environment variable "SDA_CLI" to point
# directly to the "sda-cli" executable.  If this environment variable is
# not set, the tool will be picked up from "$PATH" as usual.
SDA_CLI=${SDA_CLI-sda-cli}

encrypt () {
	# Encrypt the given files using "sda-cli".
	#
	# Files are encrypted unconditionally, regardless of whether
	# there exists encrypted variants of the files or not.  The only
	# scenario wherein a file is not encrypted is when the user
	# gives us the pathname of an encrypted file and we can't find
	# the unencrypted variant of the file by simply removing the
	# ".c4gh" filename suffix.
	#
	# Directories and other non-regular files are ignored.

	for pathname do
		shift

		if [ ! -f "$pathname" ] || [ ! -f "${pathname%.c4gh}" ]
		then
			# Skip if we are given something that doesn't
			# exist or isn't a regular file, or if the
			# variant of the filename with no ".c4gh" suffix
			# does not exist or isn't a regular file.
			continue
		fi

		pathname=${pathname%.c4gh}

		# Skip if the unencrypted pathname is already in the
		# list.
		for dup do
			if [ "$pathname" = "$dup" ]; then
				continue 2
			fi
		done

		# Remove the encrypted variant of the file, if it
		# exists.
		rm -f "$pathname.c4gh"

		# Remember the unencrypted variant of the file for
		# encryption later.
		set -- "$@" "$pathname"
	done

	# If there are files to encrypt, encrypt them.
	if [ "$#" -gt 0 ]; then
		"$SDA_CLI" encrypt -key "$SDA_KEY" "$@"
	fi
}

upload () {
	# Encrypt+upload using "sda-cli".
	#
	# Files are uploaded to the top-level directory of the S3
	# storage bucket, offset by the target directory path given by
	# the option-argument of the "-t" option.
	#
	# Directories are handled recursively and will be uploaded to
	# the target directory path given by the directory name, offset
	# by the target path given by the option-argument of the "-t"
	# option.

	OPTIND=1
	unset -v target_dir
	while getopts t: opt; do
		case $opt in
			t)
				target_dir=$OPTARG
				;;
			*)
				echo 'Error in command line parsing' >&2
				exit 1
		esac
	done
	shift "$(( OPTIND - 1 ))"

	# Sanity check the target directory path.
	case ${target_dir-} in
		*..*)
			echo 'Target path contains ".."' >&2
			exit 1
			;;
		/*)
			echo 'Target path is absolute' >&2
			exit 1
	esac

	for pathname do
		shift
		if [ -d "$pathname" ]; then
			# Recursively encrypt and upload the directory.
			# We do this in a subshell to isolate the
			# changes made to the "target_dir" variable in
			# the recursive call.
			(
				upload -t "${target_dir-.}/$(basename "$pathname")" \
					"$pathname"/*
			)
			continue
		fi
		set -- "$@" "$pathname"
	done

	encrypt "$@"

	# Ensure that our list of files to upload only consists of
	# encrypted files that exists, and that this list does not
	# contain duplicate entries.
	for pathname do
		shift
		pathname=${pathname%.c4gh}.c4gh

		if [ ! -f "$pathname" ]; then
			continue
		fi

		for dup do
			if [ "$pathname" = "$dup" ]; then
				continue 2
			fi
		done

		set -- "$@" "$pathname"
	done

	# If there are files to upload, upload them, possibly in a
	# subdirectory of the user's S3 bucket.
	if [ "$#" -gt 0 ]; then
		"$SDA_CLI" upload \
			-config "$SDA_CONFIG" \
			${target_dir+-targetDir "$target_dir"} \
			"$@"
	fi
}

curl () {
	# Helper function that makes curl calls a bit shorter.

	command curl --silent \
		--request POST \
		--user "$MQ_CREDENTIALS" \
		--header "Content-Type: application/json" \
		"$@"
}

access_key () {
	# Parses the S3 configuration file and outputs the access key.

	sed	-e '/^access_key[[:blank:]]*=[[:blank:]]*/!d' \
		-e 's///' -e 's/[[:blank:]]*$//' -e 'q' \
		"$SDA_CONFIG"
}

publish () {
	# Will read base64-encoded messages from standard input, one
	# per line, decode each message and publish it.  Any output is
	# explicitly discarded.

	while IFS= read -r message; do
		printf "%s\n" "$message" | base64 -d |
		curl --data @- "$url_exchanges/publish"
	done >/dev/null
}

jq_filter=$(cat <<'JQ_FILTER'
map(
	# Add an array of pathnames that would match this message.  This
	# includes the pathname of each parent directory, leading up to
	# and including the pathname of the file itself.
	.tmp_paths = [
		# The file's full pathname is part of a base64-encoded
		# JSON blob.
		foreach (
			.payload |
			@base64d |
			fromjson.filepath |
			split("/")[]
		) as $elem (
			null;
			. += $elem + "/";
			.
		)
	] |
	# The last element is the full file path and should not have a
	# trailing slash.
	.tmp_paths[-1] |= rtrimstr("/")
) |
[
	# Match the pathnames given as positional arguments against the
	# computed pathnames in the "tmp_paths" array in each message.
	# Depending on the $yes boolean variable, extract or discard
	# matching messages.
	JOIN(
		INDEX($ARGS.positional[]; .);
		.[];
		.tmp_paths[];
		if (.[1:] | any | if $yes then . else not end) then
			.[0]
		else
			empty
		end
	)
] |
# Deduplicate the extracted messages on the full pathname of the file,
# then remove the "tmp_paths" array from each message and base64 encode
# them.
unique_by(.tmp_paths[-1]) |
map( del(.tmp_paths) | @base64 )[]
JQ_FILTER
)

get_messages () {
	# Retrieves the messages on the queue given by the 1st argument.
	# The remaining arguments are pathnames that we filter the
	# messages with (together with the access key from the S3
	# configuration).  Any message that does not correspond to any
	# of the given pathnames is requeued.  The remaining messages
	# are individually base64-encoded and outputted on the standard
	# output stream, one message per line of output.
	#
	# If a given pathname ends with a slash, then all messages with
	# file paths in or below that directory will be returned.  If
	# the given pathname is an empty string (""), then all messages
	# are returned.

	queue=$1
	shift

	access_key=$(access_key)

	for pathname do
		shift
		pathname=$access_key/$pathname
		set -- "$@" "$pathname"
	done

	tmpfile=$(mktemp)
	# shellcheck disable=SC2064
	trap "rm '$tmpfile'" EXIT

	# Get upload messages and ACK them all without requeuing them.
	# This empties the queue.
	#
	curl --data '{"count":-1,"encoding":"base64","ackmode":"ack_requeue_false"}' \
		"$url_queues/$queue/get" >"$tmpfile"

	# Requeue the messages that we're not interested in.
	#
	# Note that we only requeue unique messages, based on the file
	# path stored in each message's payload.
	#
	jq -r --argjson yes false "$jq_filter" --args "$@" <"$tmpfile" |
	publish

	# Filter out (extract) the the set of messages that we want to
	# keep.  This set does not contain any duplicated file paths.
	#
	jq -r --argjson yes true "$jq_filter" --args "$@" <"$tmpfile"

	rm -f "$tmpfile"
	trap - EXIT
}

get_filenames () {
	# Return the filenames present in the queue given by the 1st
	# argument.  The messages in the queue are filtered on the
	# access key from the S3 configuration.

	queue=$1

	access_key=$(access_key)

	curl --data \
		'{"count":-1,"encoding":"base64","ackmode":"ack_requeue_true"}' \
		"$url_queues/$queue/get" |
	jq -r --arg access_key "$access_key" '
		map(.payload | @base64d | fromjson |
		select(.filepath | startswith($access_key + "/")).filepath |
			sub(".*?/"; "")) | unique[]'
}

ingest () {
	# Ingest the given filenames.  If given a directory path ending
	# with a slash, ingest all files in or below that path.

	# If no arguments are given, list the filenames that may be
	# processed, then return immediately.
	#
	if [ "$#" -eq 0 ]; then
		get_filenames inbox
		return
	fi

	# Get the messages that we want from the "inbox" queue, then
	# rewrite them into ingest messages and publish them.
	#
	get_messages inbox "$@" |
	jq -r -R '@base64d | fromjson |
		.payload |= (
			@base64d | fromjson |
			.type = "ingest" |
			del(.filesize,.operation) |
			@base64
		) |
		.routing_key = "ingest" |
		del(.payload_bytes) |
		@base64' |
	publish
}

accession () {
	# Assign accession IDs to ingested files.

	# If no arguments are given, list the filenames that may be
	# processed, then return immediately.
	#
	if [ "$#" -eq 0 ]; then
		get_filenames verified
		return
	fi

	# We expect exactly two arguments here; one accession ID and one
	# filename.  The filename should not end in a slash.
	#
	if	[ "$#" -ne 2 ] ||
		case $2 in (*/) true;; (*) false; esac
	then
		usage_accession >&2
		return 1
	fi

	accession_id=$1
	shift

	# Get the message that we want from the "verified" queue (there
	# will be at most one message as they are deduplicated based
	# on the file path, and we're only querying using a single
	# filename), then rewrite them into accession messages and
	# publish them.
	#
	get_messages verified "$@" |
	jq -r -R --arg accession_id "$accession_id" '
		@base64d | fromjson |
		.payload |= (
			@base64d | fromjson |
			.type = "accession" |
			.accession_id = $accession_id |
			del(.filesize,.operation) |
			@base64
		) |
		.routing_key = "accessionIDs" |
		del(.payload_bytes) |
		@base64' |
	publish
}

dataset () {
	# Collect filenames into datasets.  If a directory path is
	# given with a slash at the end, all files in or beneath that
	# directory will be assigned to the given dataset ID.

	# If no arguments are given, list the files that may be
	# processed, then return immediately.
	#
	if [ "$#" -eq 0 ]; then
		get_filenames completed
		return
	fi

	# We expect at least two arguments here; one dataset ID and at
	# least one filename.
	#
	if [ "$#" -lt 2 ]; then
		usage_dataset >&2
		return 1
	fi

	dataset_id=$1
	shift

	# Get the messages that we want from the "completed" queue, and
	# assign the accession IDs from these to the given dataset ID.
	#
	get_messages completed "$@" |
	jq -r -R -n --arg dataset_id "$dataset_id" '
		{
			properties: {
				delivery_mode: 2,
				content_encoding: "UTF-8",
				content_type: "application/json"
			},
			routing_key: "mappings",
			payload: {
				type: "mapping",
				dataset_id: $dataset_id,
				accession_ids: [
					inputs |
					@base64d | fromjson.payload |
					@base64d | fromjson.accession_id
				]
			} | @base64,
			payload_encoding: "base64"
		} | @base64' |
	publish
}

usage () {
	case ${1-} in
		upload|ingest|accession|dataset)
			"usage_$1"
			;;
		"")
			usage_general
			;;
		*)
			usage_general >&2
			return 1
	esac
}

usage_general () {
	cat <<-USAGE_GENERAL
	General synopsis:
	    $myself [GLOBAL OPTIONS] [help] {upload|ingest|accession|dataset} [ARGUMENTS]

	Global options:
	    --mq-credentials user:pass	MQ credentials			Currently: $MQ_CREDENTIALS
	    --mq-url URL		MQ URL				Currently: $MQ_URL
	    --sda-config pathname	SDA S3 configuration file	Currently: $SDA_CONFIG
	    --sda-key pathname		SDA CRYPT4GH public key file	Currently: $SDA_KEY

	Specific synopsis:
	    $myself help

	    $myself [...] upload [-t target-path] pathname [pathname...]
	    $myself help upload

	    $myself [...] ingest pathname [pathname...]
	    $myself [...] ingest
	    $myself help ingest

	    $myself [...] accession accessionID pathname
	    $myself [...] accession
	    $myself help accession

	    $myself [...] dataset datasetID pathname [pathname...]
	    $myself [...] dataset
	    $myself help dataset

	Environment variables:
	    SDA_CLI     Pathname of the "sda-cli" executable.  If unset,
	                the utility will be located using your "PATH"
	                variable, as is done for any ordinary utility.

	USAGE_GENERAL
}

usage_upload () {
	cat <<-USAGE_UPLOAD
	The "upload" sub-command is used for encrypting and uploading
	one or several files or directories to the configured S3
	storage's inbox.  Any file or directory given as an operand is
	uploaded to the top-level of the S3 inbox.  Directories are
	uploaded recursively and files within them maintain their place
	in the directory structure rooted at the named directory.

	The "upload" sub-command takes an optional "-t" option whose
	option-argument will be used as a target directory path beneath
	the S3 inbox.  The given target path may not contain the
	substring ".." nor an initial "/".  See examples further down.

	Files are unconditionally re-encrypted.  Any existing encrypted
	copy of a file will be overwritten.

	This script uses "sda-cli" for encryption and uploading to the
	S3 storage.  If the "SDA_CLI" environment variable is set, it
	is assumed to hold the pathname of the "sda-cli" executable.
	If the "SDA_CLI" variable is unset, the "sda-cli" executable
	will be located using the user's "PATH" variable, like any other
	command.

	Example usage:

	    Files may be uploaded one by one or several at once.  The
	    following two commands encrypts and uploads three files
	    (this creates or re-creates "file1.c4gh", "file2.c4gh", and
	    "file3.c4gh").  All three files are placed at the top-level
	    of the inbox.

	    $myself upload file1 file2
	    $myself upload dir/file3

	    The following command encrypts and uploads all files in the
	    "data" subdirectory.  The files will retain their relative
	    location under a "data" subdirectory in the S3 inbox.

	    $myself upload data

	    Using the "-t" option, the target directory can be set to
	    some other path under the top-level inbox.  Using e.g. "-t
	    project/files" with the examples above would have the effect
	    of displacing the upload to a top-level path "project/files"
	    directory in the inbox.

	USAGE_UPLOAD
}

usage_ingest () {
	cat <<-USAGE_INGEST
	The "ingest" sub-command is used for ingesting one or several
	uploaded files.  If a directory path is specified with a
	trailing slash, all files in or beneath that directory will be
	ingested recursively.  Specifying an empty string ("") as the
	pathname will have the effect of ingesting all files in the
	user's inbox.

	Example usage:

	    Listing the filenames currently in the "inbox" queue waiting
	    to be ingested:

	    $myself ingest

	    Files may be ingested one by one or several at once.
	    Ingesting three files:

	    $myself ingest file1 file2
	    $myself ingest file3

	    Ingesting all files in or beneath the "project/data" path:

	    $myself ingest project/data/

	USAGE_INGEST
}

usage_accession () {
	cat <<-USAGE_ACCESSION
	The "accession" sub-command is used for assigning an accession ID
	to a single file that has previously been ingested.  As with the
	"ingest" sub-command, the filenames may be given with or without
	the ".c4gh" filename suffix and with or without a full directory
	path.

	Example usage:

	    Listing the filenames currently in the "verified" queue
	    waiting to have accession IDs assigned to them:

	    $myself accession

	    Accessions are only ever assigned to one file at a time.
	    Assigning accessions to three files:

	    $myself accession MYID001 file1
	    $myself accession MYID002 file2
	    $myself accession MYID003 file3

	USAGE_ACCESSION
}

usage_dataset () {
	cat <<-USAGE_DATASET
	The "dataset" sub-command is used for associating one or several
	files to a single dataset ID.  As with the "ingest" sub-command,
	the filenames may be given with or without the ".c4gh" filename
	suffix and with or without a full directory path.

	Example usage:

	    Listing the filenames currently in the "completed" queue
	    waiting to be associated with a dataset ID:

	    $myself dataset

	    Files are associated to dataset IDs one at a time or several
	    at once.  Associating three files with a dataset ID:

	    $myself dataset MYSET001 file1 file2
	    $myself dataset MYSET001 file3

	USAGE_DATASET
}

# Handle global options.
while true; do
	case ${1-} in
		--mq-credentials)
			MQ_CREDENTIALS=$2
			;;
		--mq-url)
			MQ_URL=$2
			;;
		--sda-config)
			SDA_CONFIG=$2
			;;
		--sda-key)
			SDA_KEY=$2
			;;
		*)
			break
	esac
	shift 2
done

url_api=$MQ_URL/api
url_exchanges=$url_api/exchanges/gdi/sda
url_queues=$url_api/queues/gdi

# Handle sub-commands.
case ${1-} in
	upload|ingest|accession|dataset)
		"$@"
		;;
	help)
		shift
		usage "$@"
		;;
	*)
		usage >&2
		exit 1
esac
